{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text = [\n",
        "\n",
        "    \"rnn\", \"learns\", \"from\", \"history\",\n",
        "    \"rnn\", \"is\", \"powerful\", \"model\",\n",
        "    \"rnn\", \"works\", \"on\", \"time-series\",\n",
        "    \"rnn\", \"finds\", \"patterns\", \"sequences\",\n",
        "    \"rnn\", \"has\", \"memory\", \"capabilities\",\n",
        "    \"rnn\", \"is\", \"good\", \"for\",\n",
        "    \"rnn\", \"predicts\", \"future\", \"events\",\n",
        "     \"rnn\", \"is\", \"used\", \"widely\",\n",
        "    \"rnn\", \"helps\", \"with\", \"sequences\",\n",
        "    \"rnn\", \"can\", \"process\", \"data\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "word_to_index = {word: i for i, word in enumerate(sorted(set(text)))}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "vocab_size = len(word_to_index)\n",
        "seq_length = 3\n",
        "\n",
        "def prepare_data(text, word_to_index, seq_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        seq = [word_to_index[word] for word in text[i:i+seq_length]]\n",
        "        target = word_to_index[text[i+seq_length]]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "def one_hot_encode(sequences, vocab_size):\n",
        "    encoded = np.zeros((sequences.shape[0], sequences.shape[1], vocab_size))\n",
        "    for i, seq in enumerate(sequences):\n",
        "        for j, word_index in enumerate(seq):\n",
        "            encoded[i, j, word_index] = 1\n",
        "    return encoded\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def cross_entropy(pred, true_index):\n",
        "    return -np.log(pred[true_index] + 1e-9)\n",
        "\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, lr=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.lr = lr\n",
        "\n",
        "        self.Wx = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.Wh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Wy = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.bh = np.zeros((1, hidden_size))\n",
        "        self.by = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        T = X.shape[0]\n",
        "        self.h = np.zeros((T, self.hidden_size))\n",
        "        self.o = np.zeros((T, self.output_size))\n",
        "        for t in range(T):\n",
        "            if t == 0:\n",
        "                self.h[t] = np.tanh(X[t].dot(self.Wx) + self.bh)\n",
        "            else:\n",
        "                self.h[t] = np.tanh(X[t].dot(self.Wx) + self.h[t-1].dot(self.Wh) + self.bh)\n",
        "            self.o[t] = self.h[t].dot(self.Wy) + self.by\n",
        "        return self.o\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        probs = softmax(output[-1])\n",
        "        return np.argmax(probs)\n",
        "\n",
        "    def train(self, X_train, y_train, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(X_train.shape[0]):\n",
        "                X = X_train[i]\n",
        "                y_true = y_train[i]\n",
        "\n",
        "                # FORWARD\n",
        "                o = self.forward(X)\n",
        "                probs = softmax(o[-1])\n",
        "                loss = cross_entropy(probs, y_true)\n",
        "                total_loss += loss\n",
        "\n",
        "                # BACKWARD\n",
        "                dWy = np.outer(self.h[-1], probs)\n",
        "                dWy[:, y_true] -= self.h[-1]\n",
        "\n",
        "                dby = probs.copy()\n",
        "                dby[y_true] -= 1\n",
        "\n",
        "                dh = probs.copy()\n",
        "                dh[y_true] -= 1\n",
        "                dh = dh.dot(self.Wy.T)\n",
        "\n",
        "                dWx = np.zeros_like(self.Wx)\n",
        "                dWh = np.zeros_like(self.Wh)\n",
        "                dbh = np.zeros_like(self.bh)\n",
        "                dh_next = np.zeros_like(dh)\n",
        "\n",
        "                for t in reversed(range(seq_length)):\n",
        "                    h_raw = (1 - self.h[t] ** 2) * (dh + dh_next)\n",
        "                    dWx += np.outer(X[t], h_raw)\n",
        "                    dbh += h_raw\n",
        "                    if t > 0:\n",
        "                        dWh += np.outer(self.h[t-1], h_raw)\n",
        "                        dh_next = h_raw.dot(self.Wh.T)\n",
        "\n",
        "\n",
        "                self.Wx -= self.lr * dWx\n",
        "                self.Wh -= self.lr * dWh\n",
        "                self.Wy -= self.lr * dWy\n",
        "                self.bh -= self.lr * dbh\n",
        "                self.by -= self.lr * dby.reshape(1, -1)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "\n",
        "sequences, targets = prepare_data(text, word_to_index, seq_length)\n",
        "sequences_one_hot = one_hot_encode(sequences, vocab_size)\n",
        "\n",
        "\n",
        "model = RNN(input_size=vocab_size, hidden_size=16, output_size=vocab_size, lr=0.1)\n",
        "model.train(sequences_one_hot, targets, epochs=1000)\n",
        "\n",
        "test_input = sequences_one_hot[0]\n",
        "pred_idx = model.predict(test_input)\n",
        "print(\"Input:\", text[0:3])\n",
        "print(\"Expected:\", text[3])\n",
        "print(\"Predicted:\", index_to_word[pred_idx])\n",
        "\n",
        "\n",
        "## made by yousef ahmed 4221244 ##\n",
        "## Rnn model ##"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUFNNtSq2ybk",
        "outputId": "40e686e0-0e08-46e7-d5bc-b07a7533deb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 121.9636\n",
            "Epoch 100, Loss: 1.0531\n",
            "Epoch 200, Loss: 0.3241\n",
            "Epoch 300, Loss: 0.1872\n",
            "Epoch 400, Loss: 0.1302\n",
            "Epoch 500, Loss: 0.0994\n",
            "Epoch 600, Loss: 0.0803\n",
            "Epoch 700, Loss: 0.0673\n",
            "Epoch 800, Loss: 0.0579\n",
            "Epoch 900, Loss: 0.0508\n",
            "Input: ['rnn', 'learns', 'from']\n",
            "Expected: history\n",
            "Predicted: history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AsMPFBv20FT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}